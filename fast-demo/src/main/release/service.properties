#hadoop
fs.defaultFS=hdfs://testcluster
dfs.nameservices=testcluster
dfs.ha.namenodes.testcluster=nn1,nn2
dfs.client.failover.proxy.provider.testcluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
dfs.namenode.rpc-address.testcluster.nn1=hadoop-auth-01:9000
dfs.namenode.rpc-address.testcluster.nn2=hadoop-auth-02:9000
dfs.client.read.shortcircuit=true
dfs.domain.socket.path=/usr/local/vipcloud/data/data1/dn._PORT
dfs.blocksize=268435456
dfs.replication=3
dfs.client.block.write.retries=10
#custom
jetty.port=8080
jetty.max.threads=100
jetty.min.threads=2
fileservice.download.project.concurrent.threads=1
fileservice.download.dir.source=/temp/download
fileservice.download.dir.unzip=/temp/unzip
#every project must conf,final path /temp/unzip/organ,/temp/download/organ
fileservice.download.project.name.organ=irtree
fileservice.download.data.progress.threshold=1048576

#every project must absolute path
hdfs.upload.path.user.personate.organ=fulh
hdfs.upload.path.delete.to.trash=true
hdfs.upload.path.project.organ=/bar/data
fileservice.download.md5.file.name=md5.txt
#download url
remote.download.url=http://192.168.30.29:8091/
#
enable.recovery=true
