<?xml version="1.0" encoding="utf-8"?>

<configuration>
	<!-- ========= modify begin ============ -->
	<property>
		<name>yarn.resourcemanager.ha.automatic-failover.zk-base-path</name>
		<value>/yarn-mrcluster-leader-election</value>
	</property>
	<property>
		<name>yarn.resourcemanager.cluster-id</name>
		<value>yarn-mrcluster-cluster</value>
	</property>
	<property>
		<name>yarn.resourcemanager.zk-address</name>
		<value>node203.vipcloud:2181,node204.vipcloud:2181,node205.vipcloud:2181</value>
	</property>
	<property>
		<name>yarn.resourcemanager.hostname.rm1</name>
		<value>Hadoop2x-01</value>
	</property>
	<property>
		<name>yarn.resourcemanager.hostname.rm2</name>
		<value>Hadoop2x-02</value>
	</property>
	<property>
		<name>yarn.resourcemanager.ha.rm-ids</name>
		<value>rm1,rm2</value>
	</property>
	<!-- ========= don't need config begin========== -->
	<property>
		<name>yarn.resourcemanager.address.rm1</name>
		<value>${yarn.resourcemanager.hostname.rm1}:8032</value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address.rm1</name>
		<value>${yarn.resourcemanager.hostname.rm1}:8030</value>
	</property>
	<property>
		<name>yarn.resourcemanager.resource-tracker.address.rm1</name>
		<value>${yarn.resourcemanager.hostname.rm1}:8031</value>
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address.rm1</name>
		<value>${yarn.resourcemanager.hostname.rm1}:8088</value>
	</property>
	<property>
		<name>yarn.resourcemanager.address.rm2</name>
		<value>${yarn.resourcemanager.hostname.rm2}:8032</value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address.rm2</name>
		<value>${yarn.resourcemanager.hostname.rm2}:8030</value>
	</property>
	<property>
		<name>yarn.resourcemanager.resource-tracker.address.rm2</name>
		<value>${yarn.resourcemanager.hostname.rm2}:8031</value>
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address.rm2</name>
		<value>${yarn.resourcemanager.hostname.rm2}:8088</value>
	</property>
	<!-- ========= don't need config end========== -->
	<property>
		<name>yarn.log.server.url</name>
		<value>http://Hadoop2x-03:19888/jobhistory</value>
		<description>job history server </description>
	</property>
	<property>
		<name>yarn.web-proxy.address</name>
		<value>Hadoop2x-03:29000</value>
		<description>The address for the web proxy as HOST:PORT, if this is
			not given then the proxy will run as part of the RM
		</description>
	</property>
	<property>
		<name>yarn.resourcemanager.nodes.include-path</name>
		<value></value>
	</property>
	<property>
		<name>yarn.resourcemanager.nodes.exclude-path</name>
		<value></value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.class</name>
		<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
	</property>
	<property>
		<name>yarn.scheduler.minimum-allocation-mb</name>
		<value>2048</value>
	</property>
	<property>
		<name>yarn.scheduler.maximum-allocation-mb</name>
		<value>97280</value>
	</property>
	<property>
		<name>yarn.scheduler.minimum-allocation-vcores</name>
		<value>1</value>
	</property>
	<property>
		<name>yarn.scheduler.maximum-allocation-vcores</name>
		<value>4</value>
	</property>
	<property>
		<name>yarn.fail-fast</name>
		<value>false</value>
	</property>
	<property>
		<name>yarn.resourcemanager.max-completed-applications</name>
		<value>5000</value>
		<description>default is 10000, The maximum number of completed
			applications RM keeps.
		</description>
	</property>
	<property>
		<name>yarn.resourcemanager.system-metrics-publisher.enabled</name>
		<value>false</value>
		<description>The setting that controls whether yarn system metrics is
			published on the timeline server or not by RM.
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.address</name>
		<value>0.0.0.0:8041</value>
	</property>
	<property>
		<name>yarn.nodemanager.recovery.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.nodemanager.recovery.dir</name>
		<value>${hadoop.tmp.dir}/data/data2/yarn-nm-recovery</value>
	</property>
	<property>
		<name>yarn.nodemanager.local-dirs</name>
		<value>${hadoop.tmp.dir}/data/data1/yarn/local,${hadoop.tmp.dir}/data/data2/yarn/local,${hadoop.tmp.dir}/data/data3/yarn/local,${hadoop.tmp.dir}/data/data4/yarn/local,${hadoop.tmp.dir}/data/data5/yarn/local,${hadoop.tmp.dir}/data/data6/yarn/local,${hadoop.tmp.dir}/data/data7/yarn/local,${hadoop.tmp.dir}/data/data8/yarn/local</value>
	</property>
	<property>
		<name>yarn.nodemanager.log-dirs</name>
		<value>${hadoop.tmp.dir}/data/data1/yarn/log,${hadoop.tmp.dir}/data/data2/yarn/log,${hadoop.tmp.dir}/data/data3/yarn/log,${hadoop.tmp.dir}/data/data4/yarn/log,${hadoop.tmp.dir}/data/data5/yarn/log,${hadoop.tmp.dir}/data/data6/yarn/log,${hadoop.tmp.dir}/data/data7/yarn/log,${hadoop.tmp.dir}/data/data8/yarn/log</value>
	</property>
	<property>
		<name>yarn.nodemanager.resource.memory-mb</name>
		<value>97280</value>
	</property>
	<property>
		<name>yarn.nodemanager.pmem-check-enabled</name>
		<value>false</value>
	</property>
	<property>
		<name>yarn.nodemanager.vmem-check-enabled</name>
		<value>false</value>
	</property>
		<property>
		<name>yarn.nodemanager.vmem-pmem-ratio</name>
		<value>2.1</value>
	</property>
	<property>
		<name>yarn.nodemanager.resource.cpu-vcores</name>
		<value>45</value>
		<description> Number of vcores that can be allocated for containers.
			This is used by the RM scheduler when allocating resources for
			containers. This is not used to limit the number of physical cores
			used by YARN containers.
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.resource.percentage-physical-cpu-limit</name>
		<value>90</value>
		<description>Percentage of CPU that can be allocated for containers.
			This setting allows users to limit the amount of CPU that YARN
			containers use. Currently functional only on Linux using cgroups. The
			default is to use 100% of CPU. </description>
	</property>
	<property>
		<name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
		<value>0.25</value>
		<description>The minimum fraction of number of disks to be healthy for
			the nodemanager to launch new containers. This correspond to both
			yarn-nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e. If
			there are less number of healthy local-dirs (or log-dirs) available,
			then new containers will not be launched on this node.
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
		<value>98.0</value>
		<description> The maximum percentage of disk space utilization allowed
			after which a disk is marked as bad. Values can range from 0.0 to
			100.0. If the value is greater than or equal to 100, the nodemanager
			will check for full disk. This applies to yarn-nodemanager.local-dirs
			and yarn.nodemanager.log-dirs.
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.disk-health-checker.disk-utilization-watermark-low-per-disk-percentage</name>
		<value></value>
		<description>The low threshold percentage of disk space used when a
			bad disk is marked as good. Values can range from 0.0 to 100.0. This
			applies to yarn-nodemanager.local-dirs and yarn.nodemanager.log-dirs.
			Note that if its value is more than
			yarn.nodemanager.disk-health-checker.
			max-disk-utilization-per-disk-percentage or not set, it will be set
			to the same value as
			yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage.
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb
		</name>
		<value>4096</value>
		<description> The minimum space that must be available on a disk for
			it to be used. This applies to yarn-nodemanager.local-dirs and
			yarn.nodemanager.log-dirs.
		</description>
	</property>
	<!-- ========= modify end ============ -->
	<property>
		<name>yarn.node-labels.manager-class</name>
		<value>org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager</value>
	</property>
	<property>
		<name>yarn.node-labels.fs-store.root-dir</name>
		<value>${fs.defaultFS}/yarn/node-labels</value>
	</property>
	<property>
		<name>yarn.resourcemanager.fs.state-store.uri</name>
		<value>/yarn/system/rmstore</value>
	</property>
	<property>
		<name>yarn.resourcemanager.connect.retry-interval.ms</name>
		<value>2000</value>
	</property>
	<property>
		<name>yarn.resourcemanager.ha.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.resourcemanager.recovery.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.resourcemanager.store.class</name>
		<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore</value>
	</property>
		<property>
		<name>yarn.resourcemanager.work-preserving-recovery.enabled</name>
		<value>true</value>
		<description> Enable RM work preserving recovery. This configuration
			is private to YARN for experimenting the feature.
		</description>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.monitor.enable</name>
		<value>false</value>
		<description> Enable a set of periodic monitors (specified in
			yarn.resourcemanager.scheduler.monitor.policies) that affect the
			scheduler.
		</description>
	</property>
	<property>
		<name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
		<value>5000</value>
	</property>

	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
		<value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	<property>
		<name>yarn.log-aggregation-enable</name>
		<value>true</value>
		<description>Whether to enable log aggregation or not. If disabled,
			NMs will keep the logs
			locally (like in 1.x) and not aggregate them
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.remote-app-log-dir</name>
		<value>${fs.defaultFS}/yarn/logs/apps</value>
		<description>
			NOTICE:<!-- hdfs path need use full path ,otherwise , the path is current 
				user sub dirs. -->
			This is on the default file-system, usually HDFS and indictes where
			the NMs should aggregate logs to. This should not be local
			file-system, otherwise
			serving daemons like history-server will not
			able to serve the aggregated
			logs.
			Default is /tmp/logs
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.remote-app-log-dir-suffix</name>
		<value>logs</value>
		<description>The remote log dir will be created at
			{yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}. Default
			value is “logs””.
		</description>
	</property>
	<property>
		<name>yarn.log-aggregation.retain-seconds</name>
		<value>604800</value>
		<description>604800 = 7 * 24 * 60 *60 s, one week.Default is -1,How
			long to wait before deleting
			aggregated-logs, -1 or a negative number
			disables the deletion of
			aggregated-logs. One needs to be careful and
			not set this to a too
			small a value so as to not burden the
			distributed file-system.
		</description>
	</property>
	<property>
		<name>yarn.log-aggregation.retain-check-interval-seconds</name>
		<value>-1</value>
		<description>Determines how long to wait between aggregated-log
			retention-checks. If it is set to 0 or a negative value, then the
			value is computed as one-tenth of the aggregated-log retention-time.
			As with the previous configuration property, one needs to be careful
			and not set this to low values. Defaults to -1.
		</description>
	</property>
	<property>
		<name>yarn.nodemanager.container.cpu.separate.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>yarn.nodemanager.container.cpu.separate.command</name>
		<value>/bin/taskset -c 2-23</value>
	</property>
</configuration>
