<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<property>
		<name>dfs.namenode.logging.level</name>
		<value>info</value>
		<description>
			The logging level for dfs namenode. Other values are
			"dir" (trace namespace mutations), "block"(trace block under/over
			replications and block creations/deletions), or "all".
		</description>
	</property>

	<property>
		<name>dfs.name.dir</name>
		<value>${hadoop.tmp.dir}/data/dfs/name,${hadoop.tmp.dir}/data2/dfs/name</value>
		<description>
			Determines where on the local filesystem the DFS name
			node
			should store the name table(fsimage). If this is a
			comma-delimited list
			of directories then the name table is replicated
			in all of the
			directories, for redundancy.
		</description>
	</property>

	<property>
		<name>fs.checkpoint.dir</name>
		<value>${hadoop.tmp.dir}/data1/dfs/nosql_namesecondary,${hadoop.tmp.dir}/data2/dfs/nosql_namesecondary</value>
	</property>

	<property>
		<name>fs.checkpoint.edits.dir</name>
		<value>${hadoop.tmp.dir}/data1/dfs/nosql_namesecondary,${hadoop.tmp.dir}/data2/dfs/nosql_namesecondary</value>
	</property>

	<property>
		<name>dfs.permissions</name>
		<value>false</value>
		<description>
			If "true", enable permission checking in HDFS.
			If "false",
			permission checking is turned off,
			but all other behavior is
			unchanged.
			Switching from one parameter value to the other does not
			change the mode,
			owner or group of files or directories.
		</description>
	</property>

	<property>
		<name>dfs.data.dir</name>
	<value>${hadoop.tmp.dir}/data/dfs/data,${hadoop.tmp.dir}/data2/dfs/data</value>
		<description>
			Determines where on the local filesystem an DFS data node
			should store its blocks. If this is a comma-delimited
			list of
			directories, then data will be stored in all named
			directories,
			typically on different devices.
			Directories that do not exist are
			ignored.
		</description>
	</property>

	<property>
		<name>dfs.namenode.handler.count</name>
		<value>100</value>
		<description>The number of server threads for the namenode.
		</description>
	</property>

	<property>
		<name>dfs.replication.considerLoad</name>
		<value>true</value>
		<description>Decide if chooseTarget considers the target's load or not
		</description>
	</property>

	<property>
		<name>dfs.datanode.du.reserved</name>
		<value>10737418240</value>
		<description>Reserved space in bytes per volume. Always leave this
			much space free for non dfs use.
		</description>
	</property>

	<property>
		<name>dfs.datanode.failed.volumes.tolerated</name>
		<value>1</value>
		<description>The number of volumes that are allowed to
			fail before a
			datanode stops offering service. By default
			any volume failure will
			cause a datanode to shutdown.
                </description>
	</property>

	<property>
		<name>dfs.df.interval</name>
		<value>60000</value>
		<description>Disk usage statistics refresh interval in msec.
		</description>
	</property>

	<property>
		<name>dfs.blockreport.intervalMsec</name>
		<value>3600000</value>
		<description>Determines block reporting interval in milliseconds.
		</description>
	</property>

	<property>
		<name>dfs.replication</name>
		<value>2</value>
		<description>
			Default block replication. The actual number of
			replications can be specified when the file is
			created. The default is
			used if replication is not specified in create time.
		</description>
	</property>

	<property>
		<name>dfs.replication.interval</name>
		<value>3</value>
		<description>The periodicity in seconds with which the namenode
			computes
			repliaction work for datanodes.
		</description>
	</property>

	<property>
		<name>dfs.block.size</name>
		<value>268435456</value>
		<description>
			The default block size for new files.
			Default is
			67108864(64M).268435456(256M)
		</description>
	</property>

	<property>
		<name>dfs.datanode.handler.count</name>
		<value>100</value>
		<description>
			The number of server threads for the datanode.
		</description>
	</property>

	<property>
		<name>dfs.datanode.max.xcievers</name>
		<value>40000</value>
	</property>

	<property>
		<name>dfs.balance.bandwidthPerSec</name>
		<value>52428800</value>
		<description>
			Specifies the maximum amount of bandwidth that each
			datanode can utilize for the balancing purpose in term of the number
			of bytes per second. now ,is 50M/s, default is 1 MB/s
		</description>
	</property>


	<property>
		<name>dfs.datanode.directoryscan.threads</name>
		<value>4</value>
		<description>Number of threads to use when scanning volumes to
			generate block reports. A value greater than one means means
			volumes
			will be scanned in parallel.
		</description>
	</property>

	<property>
		<name>dfs.heartbeat.interval</name>
		<value>3</value>
		<description>Determines datanode heartbeat interval in seconds.
		</description>
	</property>

	<property>
		<name>dfs.hosts</name>
		<value></value>
		<description>Names a file that contains a list of hosts that are
			permitted to connect to the namenode. The full pathname of the file
			must be specified. If the value is empty, all hosts are
			permitted.
		</description>
	</property>

	<property>
		<name>dfs.hosts.exclude</name>
		<value></value>
		<description>Names a file that contains a list of hosts that are
			not
			permitted to connect to the namenode. The full pathname of the
			file
			must be specified. If the value is empty, no hosts are
			excluded.
		</description>
	</property>



	<property>
		<name>dfs.support.append</name>
		<value>true</value>
		<description>Does HDFS allow appends to files?</description>
	</property>

	<property>
		<name>dfs.block.local-path-access.user</name>
		<value>vipcloud,root</value>
	</property>
	<!-- General HDFS security config -->
	<property>
		<name>dfs.https.address</name>
		<value>0.0.0.0:50470</value>
	</property>
	<property>
		<name>dfs.https.port</name>
		<value>50470</value>
	</property>
	<property>
  		<name>dfs.namenode.keytab.file</name>
  		<value>/usr/local/vipcloud/hadoop/conf/vipcloud.keytab</value>
	</property>
	<property>
  		<name>dfs.namenode.kerberos.principal</name>
  		<value>vipcloud/_HOST@CQVIP.COM</value>
	</property>
	<property>
		<name>dfs.namenode.kerberos.https.principal</name>
		<value>HTTP/_HOST@CQVIP.COM</value>
	</property>
	<!-- Secondary NameNode security config -->
	<property>
		<name>dfs.secondary.https.address</name>
		<value>0.0.0.0:50495</value>
	</property>
	<property>
		<name>dfs.secondary.https.port</name>
		<value>50495</value>
	</property>
	<property>
		<name>dfs.secondary.namenode.keytab.file</name>
		<value>/usr/local/vipcloud/hadoop/conf/vipcloud.keytab</value>
	</property>
	<property>
	  	<name>dfs.secondary.namenode.kerberos.principal</name>
	  	<value>vipcloud/_HOST@CQVIP.COM</value>
	</property>
	<property>
	  	<name>dfs.secondary.namenode.kerberos.https.principal</name>
	  	<value>HTTP/_HOST@CQVIP.COM</value>
	</property>
	<!-- DataNode security config -->
	<property>
  		<name>dfs.datanode.data.dir.perm</name>
  		<value>700</value> 
	</property>
	<property>
  		<name>dfs.datanode.address</name>
  		<value>0.0.0.0:1004</value>
	</property>
	<property>
  		<name>dfs.datanode.http.address</name>
  		<value>0.0.0.0:1006</value>
	</property>
	<property>
	  	<name>dfs.datanode.keytab.file</name>
	  	<value>/usr/local/vipcloud/hadoop/conf/vipcloud.keytab</value>
	</property>
	<property>
	  	<name>dfs.datanode.kerberos.principal</name>
	  	<value>vipcloud/_HOST@CQVIP.COM</value>
	</property>
	<property>
		<name>dfs.datanode.kerberos.https.principal</name>  
		<value>HTTP/_HOST@CQVIP.COM</value>
	</property>
</configuration>
