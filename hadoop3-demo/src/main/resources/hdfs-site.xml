<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Licensed under the Apache License, Version 2.0 (the "License"); you 
	may not use this file except in compliance with the License. You may obtain 
	a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless 
	required by applicable law or agreed to in writing, software distributed 
	under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES 
	OR CONDITIONS OF ANY KIND, either express or implied. See the License for 
	the specific language governing permissions and limitations under the License. 
	See accompanying LICENSE file. -->

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<!--=============== modify begin================= -->
	<property>
		<name>dfs.namenode.name.dir</name>
		<!--must create dir path -->
		<value>${hadoop.tmp.dir}/data/data1/dfs/name</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<!--must create dir path -->
		<value>${hadoop.tmp.dir}/data/data1/dfs/data</value>
	</property>
	<property>
		<name>dfs.datanode.max.transfer.threads</name>
		<value>32768</value>
	</property>
	<property>
		<name>dfs.blocksize</name>
		<value>268435456</value>
		<description>
			The default block size for new files.
			Default is
			536870912(512M),268435456(256M)
		</description>
	</property>
	<property>
		<name>dfs.datanode.du.reserved</name>
		<value>10737418240</value>
		<description>Reserved space in bytes per volume. Always leave this
			much space free for non dfs use.10737418240(10G)
		</description>
	</property>
	<property>
		<name>dfs.datanode.failed.volumes.tolerated</name>
		<value>0</value>
		<description>The number of volumes that are allowed to
			fail before a
			datanode stops offering service. By default
			any volume failure will
			cause a datanode to shutdown.
		</description>
	</property>
	<property>
		<name>dfs.hosts</name>
		<value></value>
	</property>
	<property>
		<name>dfs.hosts.exclude</name>
		<value>/usr/local/vipcloud/hadoop/etc/hadoop/hosts.exclude</value>
	</property>
	<property>
		<name>dfs.namenode.support.allow.format</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.datanode.directoryscan.threads</name>
		<value>1</value>
	</property>
	<property>
		<name>dfs.datanode.balance.max.concurrent.moves</name>
		<value>300</value>
	</property>
	<!--=============== modify end================= -->

	<!--===============shortcircuit and ha modify begin================= -->
	<property>
		<name>dfs.client.read.shortcircuit</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.domain.socket.path</name>
		<!--must create dir path -->
		<value>${hadoop.tmp.dir}/data/data1/dn._PORT</value>
	</property>
	<property>
		<name>dfs.client.file-block-storage-locations.timeout.millis</name>
		<value>10000</value>
	</property>
	<property>
		<name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
		<value>true</value>
	</property>
	<!--ha begin -->
	<property>
		<name>dfs.nameservices</name>
		<value>testcluster</value>
	</property>
	<property>
		<name>dfs.ha.namenodes.testcluster</name>
		<value>nn1,nn2</value>
	</property>
	<!-- custom hostname var -->
	<property>
		<name>dfs.namenode.hostname.nn1</name>
		<value>hadoop-auth-01</value>
	</property>
	<property>
		<name>dfs.namenode.hostname.nn2</name>
		<value>hadoop-auth-02</value>
	</property>
	<property>
		<name>dfs.namenode.rpc-address.testcluster.nn1</name>
		<value>${dfs.namenode.hostname.nn1}:9000</value>
	</property>
	<property>
		<name>dfs.namenode.rpc-address.testcluster.nn2</name>
		<value>${dfs.namenode.hostname.nn2}:9000</value>
	</property>
	<property>
		<name>dfs.namenode.http-address.testcluster.nn1</name>
		<value>${dfs.namenode.hostname.nn1}:50070</value>
	</property>
	<property>
		<name>dfs.namenode.http-address.testcluster.nn2</name>
		<value>${dfs.namenode.hostname.nn2}:50070</value>
	</property>
	<property>
		<name>dfs.namenode.shared.edits.dir</name>
		<value>qjournal://hadoop-auth-01:8485;hadoop-auth-02:8485;hadoop-auth-03:8485/testtestcluster</value>
	</property>
	<property>
		<name>dfs.journalnode.edits.dir</name>
		<!--must create dir path -->
		<value>${hadoop.tmp.dir}/data/data1/dfs/journal</value>
	</property>
	<!--===============shortcircuit and ha modify end================= -->
	<property>
		<name>dfs.client.failover.proxy.provider.testcluster</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
	<property>
		<name>dfs.ha.fencing.methods</name>
		<value>shell(true)</value>
	</property>
	<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/root/.ssh/id_rsa</value>
	</property>
	<property>
		<name>dfs.ha.fencing.ssh.connect-timeout</name>
		<value>30000</value>
		<description>
			SSH connection timeout, in milliseconds, to use with the
			builtin
			sshfence fencer.
		</description>
	</property>
	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
	</property>
	<!--ha end -->
	<property>
		<name>dfs.replication</name>
		<value>2</value>
	</property>
	<property>
		<name>dfs.support.append</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.permissions.enabled</name>
		<value>false</value>
	</property>
	<property>
		<name>dfs.namenode.acls.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.namenode.posix.acl.inheritance.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.disk.balancer.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.storage.policy.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.datanode.fsdataset.volume.choosing.policy</name>
		<value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
	</property>
	
	<property>
		<name>dfs.namenode.handler.count</name>
		<value>100</value>
		<description>The number of server threads for the namenode.
		</description>
	</property>
	 
	<property>
		<name>dfs.blockreport.intervalMsec</name>
		<value>3600000</value>
		<description>Determines block reporting interval in milliseconds.
		</description>
	</property>

	<property>
		<name>dfs.datanode.handler.count</name>
		<value>100</value>
		<description>
			The number of server threads for the datanode.
		</description>
	</property>


	 <property>
		<name>dfs.stream-buffer-size</name>
		<value>65536</value>
	</property>
	<property>
		<name>dfs.bytes-per-checksum</name>
		<value>2048</value>
	</property>
</configuration>
